# Retrieval-Augmented Generation (RAG) with Atlas Vector Search

---

<p align="center" >
    <img  width="100%" src="/assets/vector_database/8.png
    "> 
</p>

### เมื่อคุณทำงานกับโมเดลภาษา LLMs (Large Language Models) อาจพบข้อจำกัดเหล่านี้:

- ข้อจำกัดของ LLMs ข้อมูลล้าสมัย (Stale data):
LLM ถูกฝึกจากชุดข้อมูลแบบคงที่ ซึ่งตัดข้อมูลไว้ถึงแค่ช่วงเวลาหนึ่งเท่านั้น ทำให้ความรู้ที่มีอาจเก่าและไม่อัปเดต

- ไม่เข้าถึงข้อมูลเฉพาะบุคคล (No access to local data):
LLM ไม่สามารถเข้าถึงข้อมูลเฉพาะองค์กรหรือข้อมูลที่เก็บไว้ภายใน ทำให้ขาดความรู้เฉพาะด้าน

- สร้างข้อมูลผิด (Hallucinations):
เมื่อข้อมูลฝึกไม่ครบหรือไม่ทันสมัย LLM อาจตอบคำถามโดยแต่งข้อมูลขึ้นมาเอง ซึ่งอาจไม่ถูกต้อง


## ใช้ RAG (Retrieval-Augmented Generation)
คุณสามารถแก้ข้อจำกัดเหล่านี้ได้โดยใช้สถาปัตยกรรม RAG ซึ่งทำงานตามขั้นตอนต่อไปนี้:

1. ***Ingestion*** (การนำเข้าข้อมูล):
จัดเก็บข้อมูลของคุณเองในรูปแบบเวกเตอร์ (vector embeddings) ลงในฐานข้อมูลเวกเตอร์ เช่น MongoDB Atlas Vector Search
→ เพื่อสร้างฐานความรู้ที่อัปเดตและเฉพาะตัว

2. ***Retrieval*** (การค้นหา):
เมื่อผู้ใช้ถามคำถาม ระบบจะใช้เครื่องมืออย่าง Atlas Vector Search เพื่อค้นหาเอกสารที่มีความหมายคล้ายคลึงกับคำถาม
→ แล้วนำเอกสารเหล่านั้นมาเสริมกับ LLM เพื่อใช้ในการตอบ

3. ***Generation*** (การสร้างคำตอบ):
ระบบจะส่ง prompt พร้อมเอกสารที่ค้นพบให้ LLM
→ LLM จะใช้ข้อมูลนี้เป็นบริบทในการสร้างคำตอบที่ถูกต้องและเกี่ยวข้องมากขึ้น ลดโอกาสเกิด "hallucination"


## จุดเด่นของ RAG
- ตอบคำถามได้ตรงจุดและทันสมัย
- เข้าใจข้อมูลเฉพาะองค์กรหรือโดเมน
- อัปเดตฐานความรู้ได้ตลอดเวลา
- เสริมประสิทธิภาพให้ LLM ด้วยบริบทจริง

## RAG เหมาะสำหรับอะไรบ้าง?
- ระบบถามตอบอัจฉริยะ (Q&A)
- แชทบอทที่เข้าใจข้อมูลภายในองค์กร
- การสร้างเนื้อหาพร้อมอ้างอิง
- ระบบสรุปเอกสารอัตโนมัติ
- ผู้ช่วยส่วนตัวในอุตสาหกรรมเฉพาะ (เช่น กฎหมาย การเงิน สุขภาพ)

## ข้อควรระวังและความท้าทายของ RAG
-  ใช้ทรัพยากรค่อนข้างมาก
- ทั้งการค้นหาและสร้างคำตอบต้องใช้พลังการประมวลผลสูง โดยเฉพาะกับฐานข้อมูลขนาดใหญ่
- ความซับซ้อนในการผสานระบบ
- การเชื่อมโยงส่วน retrieval และ generation ให้ทำงานร่วมกันอย่างราบรื่นไม่ใช่เรื่องง่าย
- ความน่าเชื่อถือของข้อมูลต้นทาง
- ถ้าข้อมูลที่นำเข้าไม่ดี มีอคติ หรือเก่า ก็อาจส่งผลให้คำตอบผิดพลาดได้เช่นกัน

---

## สรุป
RAG คือก้าวสำคัญในการเพิ่มประสิทธิภาพให้ LLM โดยการผสานการค้นหาข้อมูลแบบเรียลไทม์กับการสร้างคำตอบ ทำให้ระบบ AI ตอบได้แม่นยำ ทันสมัย และเข้าใจบริบทเฉพาะองค์กรมากขึ้น เหมาะอย่างยิ่งสำหรับองค์กรที่ต้องการ AI ที่ "รู้จริง" และ "เข้าใจบริบท" อย่างลึกซึ้ง